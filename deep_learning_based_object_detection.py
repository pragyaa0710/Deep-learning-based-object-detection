# -*- coding: utf-8 -*-
"""Deep Learning based object detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QF5t2UrQMaK5Nf-T3ZLEq-UDIdmPhP06
"""

# Download YOLOv3 files
!wget https://pjreddie.com/media/files/yolov3.weights
!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg
!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names

# Download Gender Detection files
!wget https://github.com/eveningglow/age-and-gender-classification/raw/master/model/gender_net.caffemodel
!wget https://github.com/eveningglow/age-and-gender-classification/raw/master/model/deploy_gender2.prototxt

# 1. INSTALL NECESSARY LIBRARIES
!pip install webcolors

# 2. DOWNLOAD THE MODEL FILES DIRECTLY INTO COLAB
import os

def download_models():
    models = {
        "yolov3.weights": "https://pjreddie.com/media/files/yolov3.weights",
        "yolov3.cfg": "https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg",
        "coco.names": "https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names",
        "gender_net.caffemodel": "https://github.com/eveningglow/age-and-gender-classification/raw/master/model/gender_net.caffemodel",
        "deploy_gender2.prototxt": "https://github.com/eveningglow/age-and-gender-classification/raw/master/model/deploy_gender2.prototxt"
    }
    for name, url in models.items():
        if not os.path.exists(name):
            print(f"Downloading {name}...")
            os.system(f"wget {url}")

download_models()

# 3. IMPORT LIBRARIES
import cv2
import numpy as np
from collections import Counter
import webcolors
from google.colab.patches import cv2_imshow
from google.colab import output
from IPython.display import display, Javascript
import PIL.Image
import io
import base64

# 4. LOAD MODELS
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
# Use GPU if available
if cv2.cuda.getCudaEnabledDeviceCount() > 0:
    print("CUDA is available. Using GPU.")
    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
else:
    print("CUDA not available. Using CPU.")
    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_DEFAULT)
    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

with open("coco.names", 'r') as f:
    labels = f.read().strip().split("\n")

gender_net = cv2.dnn.readNet("gender_net.caffemodel", "deploy_gender2.prototxt")
# Use GPU for gender_net if available
if cv2.cuda.getCudaEnabledDeviceCount() > 0:
    print("CUDA is available for gender_net. Using GPU.")
    gender_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
    gender_net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
else:
    print("CUDA not available for gender_net. Using CPU.")
    gender_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_DEFAULT)
    gender_net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

gender_list = ['Male', 'Female']

# 5. HELPER FUNCTIONS
def get_dominant_color(roi):
    if roi.size == 0: return [0, 0, 0]
    pixels = np.float32(roi.reshape(-1, 3))
    n_colors = 3
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1)
    _, labels, palette = cv2.kmeans(pixels, n_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
    dominant_color = palette[np.argmax(Counter(labels.flatten()).values())]
    return dominant_color.astype(int)

def closest_color(requested_color):
    min_colors = {}
    for name, hex_code in webcolors.CSS3_NAMES_TO_HEX.items():
        r_c, g_c, b_c = webcolors.hex_to_rgb(hex_code)
        rd = (r_c - requested_color[2]) ** 2
        gd = (g_c - requested_color[1]) ** 2
        bd = (b_c - requested_color[0]) ** 2
        min_colors[(rd + gd + bd)] = name
    return min_colors[min(min_colors.keys())]

# 6. WEBCAM JAVASCRIPT BRIDGE
def js_to_image(js_reply):
    image_bytes = base64.b64decode(js_reply.split(',')[1])
    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
    img = cv2.imdecode(jpg_as_np, flags=1)
    return img

def video_stream():
    js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;

    var pendingResolve = null;
    var shutdown = false;

    function removeDom() {
       stream.getTracks().forEach(track => track.stop());
       video.remove();
       div.remove();
       video = null;
       div = null;
    }

    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }

    async function createDom() {
      if (div !== null) return;
      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = 'max-content';
      document.body.appendChild(div);

      video = document.createElement('video');
      video.style.display = 'block';
      video.width = 640;
      video.height = 480;
      div.appendChild(video);

      stream = await navigator.mediaDevices.getUserMedia({video: {width: 640, height: 480}});
      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640;
      captureCanvas.height = 480;
      window.requestAnimationFrame(onAnimationFrame);
    }

    async function takePhoto() {
      await createDom();
      return new Promise((resolve, reject) => {
        pendingResolve = resolve;
      });
    }
    ''')
    display(js)

# 7. MAIN EXECUTION LOOP
video_stream()

while True:
    # Capture frame from webcam via JS
    js_reply = output.eval_js('takePhoto()')
    if not js_reply:
        break

    frame = js_to_image(js_reply)
    height, width = frame.shape[:2]

    # YOLO Processing
    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    layer_names = net.getLayerNames()
    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]
    layer_outputs = net.forward(output_layers)

    boxes, confidences, class_ids = [], [], []
    for output_layer in layer_outputs:
        for detection in output_layer:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x, center_y, w, h = (detection[0:4] * np.array([width, height, width, height])).astype("int")
                x, y = int(center_x - w / 2), int(center_y - h / 2)
                boxes.append([x, y, int(w), int(h)])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    if len(indices) > 0:
        for i in indices.flatten():
            x, y, w, h = boxes[i]
            x, y = max(0, x), max(0, y) # Boundary check
            label = labels[class_ids[i]]

            color_bbox = (0, 255, 0)
            cv2.rectangle(frame, (x, y), (x + w, y + h), color_bbox, 2)
            cv2.putText(frame, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_bbox, 2)

            roi = frame[y:y+h, x:x+w]
            if roi.size > 0:
                if label == "person":
                    blob_face = cv2.dnn.blobFromImage(roi, 1.0, (227, 227), (104.0, 177.0, 123.0), swapRB=False)
                    gender_net.setInput(blob_face)
                    gender = gender_list[gender_net.forward()[0].argmax()]
                    cv2.putText(frame, gender, (x, y - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_bbox, 2)
                else:
                    color_name = closest_color(get_dominant_color(roi))
                    cv2.putText(frame, color_name, (x, y - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_bbox, 2)

    # Show the resulting frame
    output.clear(wait=True)
    cv2_imshow(frame)